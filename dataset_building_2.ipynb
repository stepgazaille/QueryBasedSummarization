{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the data set\n",
    "\n",
    "In this notebook, we load data with only good pairs. First, we create the query and paragraphs embeddings using CBOW (possibly with tf-idf). \\\\\n",
    "\n",
    "Then, we only keep 200.000 lines as true pairs, and keep 400.000 additional paragraphs to wrongly associate them with the queries contained in the 200.000 previous lines to create wrong pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "#df = pd.read_csv(\"../data/para_csv.csv\")\n",
    "df = pd.read_csv(\"../data/para_csv_f1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709421, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pagename</th>\n",
       "      <th>section</th>\n",
       "      <th>para_id</th>\n",
       "      <th>para_text</th>\n",
       "      <th>rel_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0114</td>\n",
       "      <td>coverage</td>\n",
       "      <td>5c4d8a5fb15fa87ac096174957b3621b67d6a207</td>\n",
       "      <td>The 0114 dialing code includes the whole of  S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0114</td>\n",
       "      <td>history</td>\n",
       "      <td>32c808b9a9d5407d31851cad1125f3e5e4af7dda</td>\n",
       "      <td>Switching to 6-digit numbers produced 90 000 a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0114</td>\n",
       "      <td>history</td>\n",
       "      <td>4c5c5db4621532149aaf828fca282bcd5a7e757a</td>\n",
       "      <td>Before 1965  Sheffield had 5-digit telephone n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0114</td>\n",
       "      <td>history</td>\n",
       "      <td>50b5cb599e17e542fa63c14324d123bffc41cec0</td>\n",
       "      <td>Transitioning to 7-digit numbers in 1995 invol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0114</td>\n",
       "      <td>history</td>\n",
       "      <td>f2523d06b2083c7b4fc048e012460db3d2ac069d</td>\n",
       "      <td>Until the 1980s  Stocksbridge  Oughtibridge  a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pagename   section                                   para_id  \\\n",
       "0     0114  coverage  5c4d8a5fb15fa87ac096174957b3621b67d6a207   \n",
       "1     0114   history  32c808b9a9d5407d31851cad1125f3e5e4af7dda   \n",
       "2     0114   history  4c5c5db4621532149aaf828fca282bcd5a7e757a   \n",
       "3     0114   history  50b5cb599e17e542fa63c14324d123bffc41cec0   \n",
       "4     0114   history  f2523d06b2083c7b4fc048e012460db3d2ac069d   \n",
       "\n",
       "                                           para_text  rel_label  \n",
       "0  The 0114 dialing code includes the whole of  S...          1  \n",
       "1  Switching to 6-digit numbers produced 90 000 a...          1  \n",
       "2  Before 1965  Sheffield had 5-digit telephone n...          1  \n",
       "3  Transitioning to 7-digit numbers in 1995 invol...          1  \n",
       "4  Until the 1980s  Stocksbridge  Oughtibridge  a...          1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encode questions to unicode\n",
    "df['pagename'] = df['pagename'].apply(lambda x: str(x))\n",
    "df['section'] = df['section'].apply(lambda x: str(x))\n",
    "df['para_text'] = df['para_text'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding a query column which is the concatenation of pagename + section\n",
    "df['query'] = df['section'] + ' of ' + df['pagename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating an index such that all paragraphs / query associated with same pagename share the same/unique index\n",
    "ids = {}\n",
    "i = 0\n",
    "for pagename in df['pagename'].unique():\n",
    "    ids[pagename] = i\n",
    "    i+=1\n",
    "    \n",
    "df['pagename_index'] = df['pagename'].apply(lambda x: ids[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we want to train our own w2v vectors, we can adapt the code below \n",
    "'''\n",
    "import gensim\n",
    "\n",
    "questions = list(df['question1']) + list(df['question2'])\n",
    "\n",
    "# tokenize\n",
    "c = 0\n",
    "for question in tqdm(questions):\n",
    "    questions[c] = list(gensim.utils.tokenize(question, deacc=True, lower=True))\n",
    "    c += 1\n",
    "\n",
    "# train model\n",
    "model = gensim.models.Word2Vec(questions, size=300, workers=16, iter=10, negative=20)\n",
    "\n",
    "# trim memory\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# creta a dict \n",
    "w2v = dict(zip(model.index2word, model.syn0))\n",
    "print \"Number of tokens in Word2Vec:\", len(w2v.keys())\n",
    "\n",
    "# save model\n",
    "model.save('data/3_word2vec.mdl')\n",
    "model.save_word2vec_format('data/3_word2vec.bin', binary=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.feature_extraction.text import CountVectorizer\\n# merge texts\\ntext = list(df['query']) + list(df['para_text'])\\n\\ntfidf = TfidfVectorizer(lowercase=False, ) # lowercase?\\ntfidf.fit_transform(text)\\n\\n# dict key:word and value:tf-idf score\\nword2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this code to use tf-idf weighted CBOW\n",
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# merge texts\n",
    "text = list(df['query']) + list(df['para_text'])\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=False, ) # lowercase?\n",
    "tfidf.fit_transform(text)\n",
    "\n",
    "# dict key:word and value:tf-idf score\n",
    "word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''# exctract word2vec vectors\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def embed(column_name):\n",
    "    vecs = []\n",
    "    #for qu in tqdm(list(df[column_name])):\n",
    "    for qu in list(df[column_name]):\n",
    "        doc = nlp(qu) \n",
    "        mean_vec = np.zeros([len(doc), 300])\n",
    "        for word in doc:   \n",
    "            vec = word.vector\n",
    "            # only if using tf-idf\n",
    "            '''\n",
    "            # fetch df score\n",
    "            try:\n",
    "                idf = word2tfidf[str(word)]\n",
    "            except:\n",
    "                #print word\n",
    "                idf = 0\n",
    "            '''\n",
    "            # compute final vec\n",
    "            mean_vec += vec # * idf\n",
    "        mean_vec = mean_vec.mean(axis=0)\n",
    "        vecs.append(mean_vec)\n",
    "    return vecs\n",
    "    \n",
    "query_vecs = embed('query')\n",
    "paragraph_vecs = embed('para_text')\n",
    "\n",
    "df['query_CBOW'] = list(query_vecs)\n",
    "df['paragraph_CBOW'] = list(paragraph_vecs)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709421, 8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating False labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dictionnary {pagename : [list of section names]}\n",
    "pagenames = df['pagename'].values.tolist()\n",
    "sections = df['section'].values.tolist()\n",
    "d = {}\n",
    "i=0\n",
    "for pagename in pagenames:\n",
    "    if pagename not in d.keys():\n",
    "        d[pagename]=[sections[i]]\n",
    "    else:\n",
    "        d[pagename].append(sections[i])    \n",
    "    i+=1\n",
    "    \n",
    "for k in d.keys():\n",
    "    d[k] = list(set(d[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length before 671841\n",
      "number of wikipedia pages with only 1 section : 56\n",
      "length after 670887\n"
     ]
    }
   ],
   "source": [
    "# remove wikipedia pages that have only 1 section (not possible to create fake labels)\n",
    "print('length before %d' %df.shape[0])\n",
    "uniques = []\n",
    "for k in d.keys():\n",
    "    if len(d[k])==1:\n",
    "        uniques.append(k)\n",
    "print('number of wikipedia pages with only 1 section : %d' %len(uniques))\n",
    "df = df[~df.pagename.isin(uniques)]\n",
    "print('length after %d' %df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keep some good pairs\n",
    "num_true_pairs = 200000\n",
    "df_true_pairs = df[:200000]\n",
    "\n",
    "# build wrong pairs (queries from df_true_pairs associated with wrong paragraphs)\n",
    "# two times more false pairs than true pairs\n",
    "num_false_pairs = 400000\n",
    "# copy twice good queries\n",
    "df_false_pairs = df_true_pairs.append(df_true_pairs)\n",
    "df_false_pairs.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change section name\n",
    "import random \n",
    "#f = lambda x: random.choice(list(filter(lambda a: a != x[1], d[x[0]])))\n",
    "#f = lambda x: random.choice(d[x[0]])\n",
    "\n",
    "def f(pagename, section): \n",
    "    # pick a random section name that belongs to the same pagename but is different from current section name\n",
    "    choices = [sec for sec in d[pagename] if sec != section]\n",
    "    return random.choice(choices)\n",
    "    \n",
    "df_false_pairs['section'] = df_false_pairs[['pagename', 'section']].apply(lambda row: f(row['pagename'], row['section']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the query\n",
    "df_false_pairs['query'] = df_false_pairs['section'] + ' of ' + df_false_pairs['pagename']\n",
    "\n",
    "# change labels to 1\n",
    "df_false_pairs['rel_label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create final dataset by merge both true and wrong pairs\n",
    "df_final = df_true_pairs.append(df_false_pairs)\n",
    "df_final.reset_index(drop=True,inplace=True)\n",
    "df_final = df_final.reindex(np.random.permutation(df_final.index))\n",
    "df_final.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 7)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'temp = df_final.head()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincentchabot/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "'temp['query_CBOW'][0][1]\n",
    "\n",
    "for i in range(len(temp['query_CBOW'][0])):\n",
    "    #print(i)\n",
    "    col_name = 'feature_'+str(i+1)\n",
    "    temp[col_name] = temp['query_CBOW'].apply(lambda x: x[i])\n",
    "    \n",
    "    \n",
    "    #df['J3'] = df.apply(lambda row:lst[row['J1']:row['J2']],axis=1)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_final.drop(['query_CBOW', 'paragraph_CBOW'],1).to_csv('../data/fold0_600K.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_final['query_CBOW'].values.dump(\"../data/fold0_600K_query_CBOW.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_final['paragraph_CBOW'].values.dump(\"../data/fold0_600K_paragraph_CBOW.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = df_final['rel_label'].values\n",
    "\n",
    "y=[]\n",
    "for i in range(len(labels)):\n",
    "    if labels[i]==1:\n",
    "        y.append([0, 1])\n",
    "    elif labels[i]==0:\n",
    "        y.append([1, 0])\n",
    "    else:\n",
    "        print('label value not in [0,1]') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_array = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[:1000]['query'].values.dump(\"../data/short_fold0_600K_query_text.csv\")\n",
    "df[:1000]['para_text'].values.dump(\"../data/short_fold0_600K_paragraph_text.csv\")\n",
    "y_array[:1000].dump(\"../data/short_fold0_600K_labels.csv\")\n",
    "df['query'].values.dump(\"../data/fold0_600K_query_text.csv\")\n",
    "df['para_text'].values.dump(\"../data/fold0_600K_paragraph_text.csv\")\n",
    "y_array.dump(\"../data/fold0_600K_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''# Creating short data set\n",
    "df_final[:1000]['query_CBOW'].values.dump(\"../data/short_fold0_600K_query_CBOW.csv\")\n",
    "df_final[:1000]['paragraph_CBOW'].values.dump(\"../data/short_fold0_600K_paragraph_CBOW.csv\")\n",
    "df_final[:1000]['query'].values.dump(\"../data/short_fold0_600K_query.csv\")\n",
    "df_final[:1000]['para_text'].values.dump(\"../data/short_fold0_600K_paragraph.csv\")\n",
    "y_array[:1000].dump(\"../data/short_fold0_600K_labels.csv\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "#q = np.array(np.load(\"../data/short_fold0_600K_query_text.csv\").tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
